{
  "agent_id": "coder4",
  "task_id": "task_4",
  "files": [
    {
      "name": "loss_functions.py",
      "purpose": "Custom loss functions",
      "priority": "medium"
    },
    {
      "name": "evaluation.py",
      "purpose": "Model evaluation and metrics",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.RO_2507.22522v1_Recognizing_Actions_from_Robotic_View_for_Natural_",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.RO_2507.22522v1_Recognizing-Actions-from-Robotic-View-for-Natural- with content analysis. Detected project type: computer vision (confidence score: 10 matches).",
    "key_algorithms": [
      "Machine",
      "Level",
      "Discriminative",
      "Supervised",
      "Comprehensive",
      "Object",
      "Baseline",
      "Scale",
      "Tection",
      "Leading"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.RO_2507.22522v1_Recognizing-Actions-from-Robotic-View-for-Natural-.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nRecognizing Actions from Robotic View for Natural Human-Robot Interaction\nZiyi Wang1Peiming Li1Hong Liu1Zhichao Deng2Can Wang3\nJun Liu4Junsong Yuan5Mengyuan Liu1\u2020\n1State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School\n2Sun Yat-sen University3Kiel University4Lancaster University5State University of New York at Buffalo\n{ziyiwang,lipeiming1001 }@stu.pku.edu.cn hongliu@pku.edu.cn dengzhch3@mail2.sysu.edu.cn\ncanwang@pku.edu.cn j.liu81@lancaster.ac.uk jsyuan@buffalo.edu liumengyuan@pku.edu.cn\nAbstract\nNatural Human-Robot Interaction (N-HRI) requires robots\nto recognize human actions at varying distances and states,\nregardless of whether the robot itself is in motion or sta-\ntionary. This setup is more flexible and practical than con-\nventional human action recognition tasks. However, exist-\ning benchmarks designed for traditional action recognition\nfail to address the unique complexities in N-HRI due to lim-\nited data, modalities, task categories, and diversity of sub-\njects and environments. To address these challenges, we in-\ntroduce ACTIVE (Action from Robotic View), a large-scale\ndataset tailored specifically for perception-centric robotic\nviews prevalent in mobile service robots. ACTIVE com-\nprises 30 composite action categories, 80 participants, and\n46,868 annotated video instances, covering both RGB and\npoint cloud modalities. Participants performed various\nhuman actions in diverse environments at distances rang-\ning from 3m to 50m, while the camera platform was also\nmobile, simulating real-world scenarios of robot percep-\ntion with varying camera heights due to uneven ground.\nThis comprehensive and challenging benchmark aims to ad-\nvance action and attribute recognition research in N-HRI.\nFurthermore, we propose ACTIVE-PC, a method that ac-\ncurately perceives human actions at long distances using\nMultilevel Neighborhood Sampling, Layered Recognizers,\nElastic Ellipse Query, and precise decoupling of kinematic\ninterference from human actions. Experimental results\ndemonstrate the effectiveness of ACTIVE-PC. Our code is\navailable at: https://github.com/wangzy01/\nACTIVE-Action-from-Robotic-View .\n1. Introduction\nA robust Natural Human-Robot Interaction (N-HRI) system\ncan provide a more personalized and intelligent interaction\n\u2020Corresponding author: Mengyuan Liu (liumengyuan@pku.edu.cn)\nActions in N-HRI\n3 -50 meters\nRaising\nRightward\nMovable Camera & LiDAR\nPhone CallStretching\nFigure 1. Data Collection Process and Samples of the ACTIVE\nDataset: The movable camera and LiDAR capture human motion\nat distances ranging from 3 to 50 meters, with scene shaking in-\ntroduced during the recording process. For visualization purposes,\nthe samples have been scaled and cropped.\nexperience by adapting to changes in user needs and evolv-\ning action patterns. It has a wide range of applications, such\nas in service robots [19], public safety surveillance [10], and\nmedical diagnostics [3, 32]. N-HRI requires robots to have\nthe ability to adapt in real-time, not only understanding hu-\nman actions but also considering changes in human position\n(e.g., walking, standing) and their own motion. Therefore,\ncompared to traditional human action recognition, human\naction recognition in N-HRI involves handling more com-\nplex human-robot interaction scenarios.\nCompared to traditional human action recognition\nvideos, videos captured from perception-centric robotic\nviews in N-HRI scenarios exhibit diverse viewpoints,\ngreater kinematic disturbances, and varied subject res-\nolutions due to changing human-robot positions, robot\nmovements, and uneven ground. While different from\nmanipulator-mounted or end-effector perspectives common\nin industrial robots, this perception-centric viewpoint aligns\nclosely with real-world applications, such as mobile assis-\ntants and patrol robots sensing humans at distances of 3\u201350\nmeters using forward-facing sensors. These challenges ne-\ncessitate specialized methods tailored specifically to thearXiv:2507.22522v1  [cs.CV]  30 Jul 2025\n\n--- Page 2 ---\nTable 1. A comparison of the ACTIVE dataset, designed for N-HRI, with other widely used general action and micro-action datasets.\nACTIVE is the first large-scale action recognition dataset specifically focused on N-HRI.\nGeneral Action Fine-Grained Action Actions from Robotic View\nDataset AttributeMSR\nAction3DMultiview\n3D EventNTU\nRGB+D 120UA V\nHumanPA\nHMDB51HOMAGE iMiGUE SMGACTIVE\n(Ours)\nAction Recog.# Videos 567 3,815 114,480 67,428 515 1,752 18,499 3,712 46,868\n# Subjects 20 10 106 119 - 27 72 40 80\n# Classes 10 10 120 155 51 75 32 16 30\nAttribute Recog. # Samples \u00d7 \u00d7 \u00d7 22,476 515 \u00d7 \u00d7 \u00d7 46,868\nData Modality D RGB+D RGB+D+IR RGB+D+IR RGB RGB+IR RGB RGB+D RGB+Point Cloud\nSensors Kinect Kinect Kinect V2 Azure DK NA RGB Cam. RGB Cam. Kinect V2 LiDAR, RGB Cam.\nCapturing\nScenarios# Sites 1 NA 3 45 NA NA NA 1 6\nOutdoor \u00d7 \u00d7 \u2713 \u2713 \u00d7 \u00d7 \u00d7 \u00d7 \u2713\nShooting Time Night \u00d7 \u00d7 \u00d7 \u2713 \u00d7 \u00d7 \u00d7 \u00d7 \u2713\nComposite Action \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u2713 \u2713 \u2713 \u2713\nDynamic Distance \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u2713\nPlatform Motion \u00d7 \u00d7 \u00d7 \u2713 \u00d7 \u00d7 \u00d7 \u00d7 \u2713\nSource CVPR ICCV TPAMI CVPR TPAMI CVPR CVPR IJCV -\nYear 2010 2013 2019 2021 2020 2021 2021 2023 -\nunique characteristics of robotic views in N-HRI.\nExisting work has already demonstrated the importance\nof using large, comprehensive, and challenging datasets to\ndevelop and evaluate state-of-the-art deep learning meth-\nods. However, in the field of human action understanding\nfor N-HRI scenarios, current datasets [13, 21] have limita-\ntions in several aspects, including: 1) Limited Sample Size:\nLarge-scale datasets are often critical for mitigating over-\nfitting and enhancing the generalization ability of models\ndeveloped on them. 2) Fixed Capture Modes: Most existing\ndatasets rely on fixed relative positions and backgrounds,\nwith subjects typically standing still and performing prede-\nfined actions or postures. In contrast, humans in N-HRI are\nusually mobile. 3) Constrained Capture Scenes: In real-\nworld applications, N-HRI needs to function in diverse en-\nvironments ( e.g., indoor, outdoor) and across various time\nperiods ( e.g., day, night). However, samples in current\ndatasets are often collected under similar conditions, which\noversimplifies the challenges in real-world N-HRI scenar-\nios. 4) Limited Capture Distances: N-HRI often involves\nfrequent changes in the human-robot position, leading not\nonly to significant motion blur but also to considerable reso-\nlution changes. Yet, in most existing datasets, humans only\nexhibit slow and subtle movements, with fixed capture dis-\ntances. 5) Limited Data Modalities: In real-world applica-\ntions, different types of sensors are often deployed to col-\nlect data under varying conditions. For instance, LiDAR\nsensors can be used for long-distance N-HRI understand-\ning, while RGB cameras are typically used for close-range\nscenes that require rich detail. This highlights the impor-\ntance of collecting multimodal data to analyze human action\nunder different conditions. However, most existing human\naction datasets only provide traditional RGB video samples\nor depth modalities. Devices like Kinect suffer from limited\ncapture range and poor performance under daylight condi-\ntions. These limitations emphasize the need for a more ro-\nbust and diverse dataset that better reflects the dynamic and\nreal-world complexities of N-HRI scenarios.\nThe aforementioned limitations of existing datasetsclearly highlight the need for a larger, more challenging,\nand comprehensive dataset for human action analysis for N-\nHRI. To this end, we introduce ACTIVE, a large-scale mul-\ntimodal dataset in this field. Specifically, ACTIVE includes\n30 action categories with disturbing motion ( e.g., Heading\n+ Nodding), involving 80 participants and attribute infor-\nmation such as gender and clothing. It consists of a total\nof 46,868 video instances, captured using sensors including\nLiDAR and RGB cameras. During data capture, partici-\npants perform various human actions in different environ-\nments (indoor and outdoor, day and night) at varying cap-\nture distances (ranging from 3m to a long-range 50m). Ad-\nditionally, we simulate the robot\u2019s motion by introducing\nmovement to the camera platform. The use of different sen-\nsors allows our dataset to provide rich data modalities, in-\ncluding both RGB and point cloud data. Overall, ACTIVE\nserves as a more challenging dataset, encompassing action\nrecognition video instances in N-HRI scenarios and intro-\nducing diverse kinematic disturbances, which can promote\nresearch on human action understanding in N-HRI settings.\nTab. 1 presents a statistical comparison between ACTIVE\nand existing datasets. More details about ACTIVE will be\ndiscussed in Sec. 3.\nAs shown in Fig. 1, unlike general actions, ACTIVE in-\ncorporates variations in human-robot states ( e.g., changes\nin relative distance and position through disturbing motion\nlike walking and camera platform movement). Recognizing\nactions from robotic view aims to understand human ac-\ntions in N-HRI scenarios, with the goal of filtering out the\ninfluence of kinematic disturbances from the robot\u2019s per-\nspective and recognizing human actions to guide N-HRI.\nThe main challenges of ACTIVE are as follows: 1) At long\ndistances ( e.g., 50 meters), human actions are subtle and\nrapid, making it even more difficult to capture the fine de-\ntails of these actions. 2) In dynamic interaction scenarios,\nlarge-scale kinematic disturbances (such as the motion of\nthe robot platform, environmental changes like lighting and\nobstacles, and the relative motion between the human and\nthe robot) further complicate action recognition.\n\n--- Page 3 ---\nRecognizing human actions from a robotic view presents\nseveral challenges, particularly when dealing with long dis-\ntances and the interference of robotic motion. Traditional\nmethods often fail to capture local details across vary-\ning scales and cannot effectively differentiate subtle hu-\nman actions from kinematic disturbances caused by robot\nplatform movements. To address these issues, we pro-\npose ACTIVE-PC, a comprehensive framework that utilizes\nMultilevel Neighborhood Sampling (MNS), Layered Rec-\nognizers (LR), and Elastic Ellipse Query (EEQ). MNS en-\nsures both global coverage and retention of critical local de-\ntails, while LR decouple features at different layers to pre-\nserve both fine-grained human actions and global kinematic\nvariations. EEQ introduces axis-specific adaptive scaling to\nhandle varying interaction distances, enhancing action dif-\nferentiation by adjusting to the spatial distribution of mo-\ntion. Experimental results demonstrate that this method im-\nproves action recognition performance in N-HRI scenarios.\n2. Related Work\n2.1. Action Recognition Datasets\nEarly 3D action recognition datasets, such as MSR-\nAction3D [13], Multiview 3D Event [31], and NTU\nRGB+D [21], have played a crucial role in advancing ac-\ntion recognition technology. The MSR-Action3D dataset\n[13] primarily consists of sequences of gaming actions\nand was one of the first to incorporate depth informa-\ntion for action analysis. Many methods have been eval-\nuated on this dataset, but due to the limited dataset size,\nrecent approaches have achieved near saturation in accu-\nracy. To address the limited sample size, the Multiview\n3D Event dataset [31] employs multiple depth sensors to\ncapture multi-view representations of the same action. The\nNTU RGB+D dataset [21], the first large-scale RGB+D ac-\ntion recognition dataset, features a broader range of sam-\nples, more intra-class variations, and more camera views,\ncapturing RGB video, depth sequences, and skeleton data.\nIn recent years, research has shifted towards fine-grained\naction recognition tasks. The iMiGUE dataset [15] aims\nto interpret athletes\u2019 emotional states by analyzing their\nmicro-gestures, recording spontaneous micro-actions after\na match. Similarly, the SMG dataset [4] focuses on both\nmicro-gesture and emotion recognition, documenting par-\nticipants\u2019 micro-actions while narrating stories. However,\naction recognition for N-HRI involves more complex and\ndiverse interaction states. Existing datasets typically rely\non fixed relative positions and backgrounds, which are not\nideal for action recognition in real-world human-machine\ninteractions. In contrast, ACTIVE is the first large-scale\naction recognition dataset focused on N-HRI, capturing a\nvariety of actions with disturbing motions in complex inter-\naction scenarios.2.2. Action Recognition Methods\nRGB-based methods, such as TSN [27], VideoMAE [24],\nUniFormer [11], and InternVideo [29], have achieved high\naccuracy in general action recognition. Similarly, various\nmethods have been developed for general action recogni-\ntion from point cloud videos [16, 22, 34]. To enhance the\nreceptive field, P4Transformer [6] employs transformers to\navoid explicit point tracking, although it does not fully en-\ncode the spatiotemporal structure. PST-Transformer [9] im-\nproves upon this by encoding features based on the spa-\ntiotemporal displacement between reference points and all\npoints in the video. However, existing point cloud-based\nmethods are typically limited to simple, single-action recog-\nnition and perform poorly on actions with disturbing mo-\ntions. Our approach focuses on recognizing actions with\ndisturbing motions in N-HRI, while remaining compatible\nwith general action recognition.\nCurrently, there is limited research specifically address-\ning micro-action recognition from point cloud videos. In\nRGB-based methods, Yonetani et al. [33] extracted hand-\ncrafted features from both first-person and third-person\nvideos for micro-action recognition. However, this ap-\nproach requires both first-person and third-person videos,\nalong with manual synchronization. Mi et al. [20] proposed\na segment-level temporal pyramid for micro-action videos.\nNonetheless, current micro-action recognition methods typ-\nically rely on fixed relative positions and backgrounds. In\ncontrast, our approach adapts to different complex interac-\ntion scenarios, making it more suitable for action recogni-\ntion in real-world human-machine interactions.\n3. Dataset\n3.1. The Action from Robotic View Dataset\nACTIVE is the first large-scale human behavior under-\nstanding dataset specifically designed for N-HRI scenarios\n(shown in Fig. 2), featuring data from 80 participants along\nwith annotated attribute information. It covers six distinct\nenvironments (including both indoor and outdoor settings,\nunder varying daylight and nighttime conditions) and in-\ncludes 30 action ( e.g., Walking forward + Waving) cate-\ngories. The dataset comprises 46,868 video instances, with\nan average duration of 3.2 seconds, totaling 41.7 hours. It\nincludes two modalities: RGB and point cloud data. The\npoint cloud data is captured using the HESAI AT128P Li-\nDAR, with a horizontal field of view of 120\u00b0, a horizontal\nangular resolution of 0.1\u00b0, a vertical field of view of 25.4\u00b0,\nand a vertical resolution of 0.2\u00b0. The scanning frame rate is\nset at 10 Hz. The RGB video has a resolution of 1280 \u00d7 720\nand a frame rate of 30 fps. Tab. 1 compares ACTIVE dataset\nwith existing human action recognition datasets. Below are\nsome key features of ACTIVE.\n\n--- Page 4 ---\n(a)Oriented ActionsA1:WalkingA2:Raising ArmsA3:GraspingA4:TouchingA5:Rubbing HandsA6:Scratching HeadA7:Touching ChinA8:Arms CrossedA9:Hands on WaistA10:StretchingA11:Shrugging ShouldersBase MotionsC1:WavingC2:Calling OverC3:Shooing AwayC4:ClappingC5:Thumbs UpC6:NoddingC7:Thumbs DownC8:Shaking HeadC9:DrinkingC10:Phone CallC11:TextingSocial Gestures\n(c)\n(b)\nArms Crossed\nB1:LeftwardB2:RightwardB3:Looking LeftB4:Looking RightB5:Turning ClockwiseB6:Turning CounterclockwiseB7:Pointing UpB8:Pointing Down\nRaising Arms\nPhone Call\nClapping\nPointing Up\nGrasping\nRightward\nLooking Left\nThumbs UpFigure 2. Details of ACTIVE: (a) Action Labels: Three categories are included: Base Motions, Oriented Actions, and Social Gestures. (b)\nThe first row illustrates the significant variations in the captured results of the same action at different human-robot distances. The second\nrow presents the raw point cloud captured by LiDAR and its corresponding RGB image. The point cloud modality retains more detailed\ngeometric information, enabling long-range N-HRI understanding with strong robustness to interference. In contrast, the RGB modality\ncomplements this by providing rich texture and semantic information at close distances. The final row showcases additional action samples\nfrom our dataset. (c) Shooting Scenes: Six shooting scenes used in ACTIVE, covering both indoor and outdoor settings, as well as day and\nnight conditions.\nDiversity of Capture States and Behavior Patterns.\nTraditional human behavior datasets typically focus on\nfixed capture modes and distances, often emphasizing a sin-\ngle behavior. In contrast, human behavior understanding\nin N-HRI requires handling more complex human-robot in-\nteraction scenarios. Therefore, ACTIVE collects data from\nsubjects performing various actions under different human-\nrobot states. In different settings, participants are instructed\nto perform multiple natural actions at varying distances.\nThe linear distance between participants and the data cap-\nture platform ranges from 3 meters to 50 meters, with addi-\ntional movement and vibrations to simulate changes in the\nrobot\u2019s state. Minor disturbances from vehicles and pedes-\ntrians are also allowed during recording.\nMore Relevant Data Modalities. Most existing human\nbehavior datasets only provide traditional RGB video sam-\nples or depth modalities. While depth data can be converted\ninto point clouds, it often lacks detailed geometric struc-ture. Additionally, devices like Kinect suffer from short\ncapture distances and poor performance under daylight con-\nditions. In contrast, point cloud data captured by LiDAR\nretains richer geometric details, supports long-range N-HRI\nunderstanding, and is more robust to interference. The RGB\nmodality complements this by offering rich texture and se-\nmantic information at close range. This underscores the\nimportance of collecting diverse data modalities. Conse-\nquently, our varied data sources are designed to foster the\ndevelopment of robust models for human behavior analysis\nin complex N-HRI scenarios.\nMultiple Human Behavior Understanding Tasks. To\ncomprehensively analyze human behavior and actions in N-\nHRI, ACTIVE currently provides annotations for two pri-\nmary tasks: human action recognition and human attribute\nrecognition. Detailed annotations for the pose estimation\ntask will be released at a later stage.\n\n--- Page 5 ---\n3.2. Benchmark Evaluations\nAction Recognition. To ensure consistency in bench-\nmarking, we define the Cross-Subject Evaluation protocol\nand use classification accuracy (percentage) as the evalua-\ntion metric. A total of 80 participants were divided into a\ntraining group and a testing group, consisting of 53 and 27\nsubjects, respectively.\nAttribute Recognition. For attribute recognition, we use\nthe same dataset partitioning strategy as for action recogni-\ntion. The performance of attribute recognition is evaluated\nby measuring the classification accuracy for each attribute.\n4. Methodologies\nThe ACTIVE dataset captures natural human-robot in-\nteractions from robotic view, encompassing both large-\nscale global motion disturbances and subtle local postural\nchanges in the human body. In this work, we primarily\naim to address two key challenges posed by the ACTIVE\ndataset: (1) At long distances, human actions are subtle\nand rapid, making it a complex task to recognize these fine\nmovements. (2) In dynamic interaction scenarios, large-\nscale global motion disturbances further increase the dif-\nficulty of action recognition. To address these challenges\nin ACTIVE, we introduce ACTIVE-PC. In Sec. 4.1, we de-\nfine the RARV task. In Sec. 4.2, we present the pipeline\nof ACTIVE-PC. In Sec. 4.3 and Sec. 4.4, we propose Mul-\ntilevel Neighborhood Sampling (MNS) and Layered Rec-\nognizers (LR) to address challenge (1) and (2), and Elastic\nEllipse Query to tackle challenge (2).\n4.1. Task Definition of RARV\nConsider an input video, X={I1, I2, . . . , I T}, where Tis\nthe length of the video. Each frame contains Npoints, and\nthe video includes both human actions ( e.g., raising arms)\nand kinematic disturbances ( e.g., human-robot proximity\nand robot movement). Our goal is to filter out the influence\nof kinematic disturbances from the robot\u2019s perspective, in\norder to obtain the human action label Y.\n4.2. Pipeline Overview\nThe pipeline of ACTIVE-PC is shown in Fig. 3. The in-\nput video X\u2208RT\u00d7N\u00d73first undergoes MNS, producing\npoint clouds with different densities: X1\u2208RT/2\u00d7N/4\u00d73,\nX2\u2208RT/2\u00d7N/16\u00d73andX3\u2208RT/2\u00d7N/32\u00d73. These point\nclouds, X1,X2andX3are then processed through EEQ for\nadaptive neighbor querying, enabling spatiotemporal track-\ning and differentiation of human actions and kinematic dis-\nturbances, resulting in feature maps F1,F2andF3.\nLow-density point clouds, which capture deeper features\nF2andF3, are passed through the Kinematic Interpreter\nfor human-robot state understanding. High-density point\nclouds, containing finer local details in F1, are input to theAction Recognizer for human-action recognition. Yhuman\nandYkinematic are fused to obtain the fine-grained action\nscoreY. The process is formulated as follows:\nX1,X2,X3=MNS (X),\nF1,F2,F3=PointTube EEQ(X1,X2,X3),\nYkinematic =Interpreter kinematic (F2,F3),\nYhuman =Recognizer human (F1),\nY=(Yhuman +Ykinematic ))\n2.(1)\n4.3. Multilevel Neighborhood Sampling\nTo address the core challenges of the ACTIVE dataset,\nwhich include the precise perception of subtle human mo-\ntions at long distances and the accurate decoupling of kine-\nmatic interference from human actions in dynamic environ-\nments, Multilevel Neighborhood Sampling (MNS, Sec. 4.3)\nand Layered Recognizers (LR, Sec. 4.4) work in tandem.\nTraditional methods typically apply multi-scale or dif-\nferent sampling rates of farthest point sampling (FPS) di-\nrectly to the entire point cloud. While this approach ensures\nglobally uniform sampling, it often neglects the correlation\nbetween local details across different scales, which is criti-\ncal in the ACTIVE dataset. In situations where the human-\nrobot distance is large, high-density local sampling can bet-\nter capture small yet distinguishable motion features.\nTo address this, we propose Multilevel Neighborhood\nSampling (MNS), which builds hierarchical dependencies\nusing local neighborhood constraints, ensuring both global\ncoverage and retention of local details. The implementa-\ntion of MNS is shown in Fig. 3. The black points represent\nthe anchor points obtained through downsampling, while\nthe red points represent the neighboring points. The pro-\ncess is as follows: First, for the input point cloud video data\nX\u2208RT\u00d7N\u00d73, we apply FPS to sample N1points globally,\nobtaining the first layer of anchor points:\nX1= FPS( X;N1). (2)\nFor the l-th layer, instead of directly sampling globally,\nwe construct a candidate set within the local neighborhood\nof the previous layer\u2019s sampled points xl\u22121,j(where j=\n1, . . . , N l\u22121). Using a radius rand a candidate number k,\nwe define the spherical neighborhood as:\nN(xl\u22121,j) =xi:|xi\u2212xl\u22121,j|2\u2264r(l),|N| \u2264 k(l).(3)\nThen, the union of the neighborhoods across all jis taken\nand duplicates are removed, forming the local candidate set:\nbXl=[\nN(xl\u22121,j). (4)\nThrough MNS, we can capture global structure in the\nsparse layers (counteracting the kinematic interference\n\n--- Page 6 ---\nClassHeading+WavingWandering+ GraspPacing + Leftward...GAPMLPAveragingTransformer GAPMLP\nGMPMLPAveragingMultilevel Neighborhood Sampling (MNS)MNSMNSTransformer Transformer \n\ud835\udefe\u00d7\ud835\udc67\ud835\udefd\u00d7\ud835\udc66\ud835\udefc\u00d7\ud835\udc651.png2.png3.png4.pngPoint tube with EEQPoint tube with EEQ\nElastic Ellipse Query (EEQ)\nMNSMNS\nFPSNeighborhoodSamplingLocalCandidateKinematic Interpreter\nAction Recognizer\nFigure 3. ACTIVE-PC architecture. First, the point cloud sequence undergoes Multilevel Neighborhood Sampling, followed by feature\nextraction using a point tube with Elastic Ellipse Query. The dense features are fed into Action Recognizer, while the sparse features are\ninput into Kinematic Interpreter. The resulting human-action and kinematic scores are then fused to obtain the fine-grained action score.\ncaused by platform motion from robotic view), while fine-\ntuning the extraction of subtle human motion changes in\nthe denser layers. This approach precisely aligns with the\nACTIVE dataset\u2019s high demand for fine-grained local infor-\nmation when capturing complex human-robot dynamics.\n4.4. Layered Recognizers\nConsidering the characteristics of the ACTIVE dataset, hu-\nman action recognition requires not only capturing fine-\ngrained, small-scale movements but also filtering out global\nkinematic variations caused by robot platform motion. To\naddress these challenges, we design Layered Recognizers,\nwhich decouples features from different layers by sending\nthem through specialized sub-networks. The core of the de-\nsign includes two branches:\nKinematic Interpreter: For features obtained from\nlarger sampling scales ( i.e., sparser layers, such as F2\nandF3), this branch focuses on global kinematic impacts\ncaused by platform motion and changes in the human-robot\nspatial relationship from the robot\u2019s perspective. Since\nthese global features are relatively dispersed in the tempo-\nral dimension, we use Global Average Pooling (GAP) to\neffectively aggregate information from all spatiotemporal\nregions, ensuring that contributions from different regions\nare considered. The process is as follows:\nYkinematic =Interpreter kinematic (F2,F3)\n=1\n2(MLP (GAP (Transformer (F2)))\n+MLP (GAP (Transformer (F3)))).(5)Human Action Recognizer: For features extracted from\nlower layers with denser point clouds ( e.g.,F1), these fea-\ntures more directly reflect subtle human actions. Here, we\nuse Global Max Pooling (GMP) to select the most signif-\nicant spatiotemporal points, emphasizing key fine-grained\naction details. The calculation is as follows:\nYhuman =Recognizer human (F1)\n=MLP (GMP (Transformer (F1)).(6)\nFinally, we fuse the classification scores from both\nbranches, averaging their contributions. This approach re-\ntains the robustness of global kinematic interpretation while\ncapturing the high discriminability of subtle action changes.\n4.5. Elastic Ellipse Query\nLet the 4D point cloud set be P={p1, p2, . . . , p N}, where\neach point pn= (xn, yn, zn)represents the neighboring\npoint nof a query point. Given a query point pqand a radius\nr, the objective of the Ball Query is to find all points whose\ndistance from the query point pqis less than or equal to r.\nLetd(pn, pq)denote the Euclidean distance between point\npnand query point pq:\nd(pn, pq) =p\n(xn\u2212xq)2+ (yn\u2212yq)2+ (zn\u2212zq)2,(7)\nwhere pq= (xq, yq, zq)is the query point.\nThe goal of Ball Query is to identify all points pnsatis-\nfying the following condition:\nB(pq, r) ={pn|d(pn, pq)\u2264r}. (8)\nThat is, the set of points within the spherical neighborhood\nB(pq, r)consists of all points pnsuch that d(pn, pq)\u2264r.\n\n--- Page 7 ---\nIn N-HRI, conventional spherical neighborhood queries\n(e.g., Ball Query) exhibit inherent limitations when con-\nfronted with kinematic-induced geometric distortions. The\nrelative motion between mobile robots and humans in-\ntroduces anisotropic deformations in the spatiotemporal\npoint cloud distribution. Three critical challenges emerge:\n1) Scale Discrepancy: Human actions ( e.g., hand ges-\ntures) manifest as localized deformations (0.1-0.5m) within\nthe robot\u2019s egocentric coordinates, while kinematic dis-\nturbances induce macroscopic displacements (up to 50m)\nalong the xy-plane. 2) Axis-wise Heterogeneity: Vertical\ndisplacements (along the z-axis) primarily reflect genuine\nhuman joint movements, while horizontal movements (in\nthe xy-plane) are more susceptible to artifacts caused by the\nrobot\u2019s self-motion and changes in the human-robot spatial\npositioning. 3) Dynamic Occlusion: Transient point cloud\ndensity variations caused by viewpoint changes violate the\nisotropic neighborhood assumption.\nFor RARV , visual similarities influenced by kinematic\ndisturbances from robotic view complicate the differentia-\ntion, causing deformations in human-actions and irregular\ndistributions in the spatiotemporal domain. Unlike kine-\nmatic disturbances, which have a large range of movement,\nhuman-actions are constrained to smaller areas. For in-\nstance, while walking involves significant movement in the\nxy-plane, human-actions like waving hands are restricted to\na smaller range. Due to the influence of kinematic, points\nwith large xy distance at different times are more likely to\nrepresent the same point, while points with large differences\nin the vertical dimension are more likely to be distinct.\nTo address these challenges, we propose the Elastic El-\nlipse Query . Considering the distribution differences be-\ntween datasets, we introduce axis-specific adaptive query\nscale learning. This mechanism establishes an anisotropic\nmetric space through learnable axis-specific scaling, en-\nabling partial tracking of points and differentiation of ac-\ntions. Given a query point pqand a neighborhood point pn,\nour adaptive distance metric is formulated as follows:\nd(pn, pq) =p\n\u03b1(xn\u2212xq)2+\u03b2(yn\u2212yq)2+\u03b3(zn\u2212zq)2, (9)\nwhere \u2126 = {\u03b1, \u03b2, \u03b3 }are trainable scaling parameters\nthat automatically adjust the neighborhood aspect ratios.\nFor the ACTIVE dataset, the final values of \u03b1,\u03b2, and \u03b3\nare(3.5632,3.6789,2.8038) , consistent with the expected\nscale ratios. This elliptic metric space provides follow-\ning distinctive advantages: 1) Kinematic-Aware Adapta-\ntion: The horizontal scaling factors ( \u03b1, \u03b2) suppress pla-\nnar displacements induced by the robot while preserving\nfine-grained human motions. In our experiments with the\nACTIVE dataset, the optimized parameters ( \u03b1= 3.56,\n\u03b2= 3.68) effectively compress the neighborhood range in\nthe xy-plane relative to the z-axis ( \u03b3= 2.80). The vertical\nscaling factor \u03b3remains sensitive to subtle posture changes,Table 2. Action recognition accuracy (%) of ACTIVE-PC on AC-\nTIVE under different dataset settings.\nMethod VenueSetting\n8\u00d7512 12 \u00d7512 12 \u00d7768\nPSTNet [7] ICLR\u201921 18.85 26.49 34.10\nP4Transformer [6] CVPR\u201921 37.97 40.56 43.88\nPSTNet++ [8] TPAMI\u201922 34.98 38.81 40.29\nPST-Transformer [9] TPAMI\u201923 41.61 42.92 46.76\n3DinAction [2] CVPR\u201924 44.88 46.23 49.19\nACTIVE-PC w/o EEQ - 52.29 53.32 55.11\nACTIVE-PC (Ours) - 58.71 59.18 60.10\nTable 3. Action recognition accuracy (%) of ACTIVE-PC on NTU\nRGB+D. CS and CV represent the cross-subject and cross-view\nevaluation protocols, respectively.\nMethod Venue CS CV\nPSTNet [7] ICLR\u20192021 90.5 96.5\nP4Transformer [6] CVPR\u20192021 90.2 96.4\nPointCMP [22] CVPR\u20192023 88.5 -\nPointCPSC [23] ICCV\u20192023 88.0 -\nPST-Transformer [9] TPAMI\u20192023 91.0 96.4\nACTIVE-PC (Ours) - 91.7 96.8\nTable 4. Action recognition accuracy (%) of ACTIVE-RGB on\nACTIVE under different dataset settings.\nMethod VenueSetting\n8\u00d7224 12 \u00d7224\nVideoSwin [17] CVPR\u201922 41.07 44.20\nMViT V2 [14] CVPR\u201922 38.15 41.83\nVideoMAE [24] NeurIPS\u201922 42.46 45.63\nUniFormer [11] TPAMI\u201923 42.55 45.60\nVideoMAE V2 [28] CVPR\u201923 45.39 47.98\nUniFormerv2 [12] ICCV\u201923 48.21 50.74\nInternVideo2 [30] ECCV\u201924 52.52 54.88\nACTIVE-RGB (Ours) - 55.90 57.56\nsuch as a 0.2-meter arm elevation, while the horizontal scal-\ning factors adaptively filter out irrelevant positional shifts.\n2) Dynamic Focus Adjustment: Unlike fixed elliptical ker-\nnels, our parameters evolve during training to handle vary-\ning interaction distances (3-50m).\n5. Experiment\nIn this section, we evaluate the performance of state-of-\nthe-art methods and ACTIVE-PC, on the ACTIVE dataset\nacross both point cloud and RGB modalities. We also con-\nduct ablation experiments on the proposed components.\n5.1. Metrics and Implementation Details\nFor evaluation, we use standard action recognition met-\nrics, specifically classification accuracy. In the point cloud\nmodality, we first apply the TransFusion[1] method to de-\ntect human subjects within the scene. The point cloud se-\nquence is then segmented into fixed-length clips, with each\nframe sampled to 768 points. For training, video-level la-\nbels are assigned as clip-level labels. The color informa-\n\n--- Page 8 ---\nTable 5. Impact of Layered Recognizers, MNS and EEQ of\nACTIVE-PC on ACTIVE.\nLayered Recognizers MNS EEQ Accuracy (%)\n\u2717 \u2717 \u2717 46.76\n\u2713 \u2717 \u2717 51.98\n\u2713 \u2713 \u2717 55.11\n\u2713 \u2717 \u2713 55.63\n\u2713 \u2713 \u2713 60.10\nTable 6. Influence of the spatial radius r. EEQ reduces the sensi-\ntivity to the hyperparameter r.\nEEQSpatial radius rsVariance0.1 0.2 0.5 1 1.5 2\n\u2717 54.83 55.11 55.39 54.52 54.21 53.51 0.456\n\u2713 59.98 60.10 60.04 59.73 59.54 59.30 0.100\nACTIVE-PCPST-TransformerWalkingRaising ArmsGraspingTouchingRubbing HandsScratching HeadTouching ChinArms CrossedHands on Waist StretchingShrugging Shoulders\nFigure 4. t-SNE distribution of PST-Transformer [6] and\nACTIVE-PC on ACTIVE dataset. Different colors represent dif-\nferent action categories, with each point corresponding to a video\nsample. All 11 categories from the Base Motions are visualized.\ntion of the points is not used. For evaluation, the video-\nlevel prediction is obtained by averaging the clip-level pre-\ndicted probabilities. The model is trained using the SGD\noptimizer for 50 epochs, with a learning rate of 0.01, de-\ncaying by a factor of 0.1 at epochs 20 and 30. For the RGB\nmodality, human bodies are detected using the object de-\ntection method [26]. Each video is evenly split into 8 or\n12 segments, with one frame randomly sampled from each\nsegment. Each frame is resized to 224 \u00d7 224. We use the\nAdamW optimizer [18] for 50 epochs, with a learning rate\nof 0.0002. All experiments are conducted on the PyTorch\nplatform, utilizing an NVIDIA RTX 4090 GPU.\n5.2. Point Cloud Modality Evaluation\nWe first validate the performance of ACTIVE-PC on the\nACTIVE dataset. As shown in Tab. 2, Transformer-based\nmodels, such as P4Transformer and PST-Transformer, out-\nperform CNN-based models achieving Acc-Top1 scores\nof 43.88% and 46.76%, respectively. ACTIVE-PC sur-\npasses PST-Transformer by 13.34%, reaching an Acc-Top1\nof 60.10%, demonstrating the effectiveness of our method.\nTo validate the robustness of the model, we conduct exper-\niments on two general action recognition datasets, MSR-\nAction3D and NTU RGB+D. Tab. 3 presents the results on\nNTU RGB+D.5.3. RGB Modality Evaluation\nACTIVE-RGB is built upon the ViT [5], augmented with\nLayered Recognizers. Tab. 4 presents a performance com-\nparison of ACTIVE-RGB with other state-of-the-art meth-\nods in the RGB modality. ACTIVE-RGB outperforms\nthe leading method, the baseline model InternVideo2, by\n2.68%, validating the effectiveness of our approach.\n5.4. Ablation Study\nEvaluation of different Components: We conduct ab-\nlation experiments on the key components of our model,\nas well as the number of sampled frames per point cloud\nvideo. Results in Tab. 5 show that each proposed modi-\nfication contributes positively. Specifically, w/o EEQ and\nw/o MNS show performance drops of 4.99% and 4.47%,\nrespectively, compared to ACTIVE-PC. The significant per-\nformance degradation observed with w/o Layered Recog-\nnizers highlights the effectiveness of fusing high-frequency\nand low-frequency features. Allowing the model to simul-\ntaneously focus on both action and kinematic variations en-\nhances its ability to handle complex environments.\nEvaluation of the Stability of EEQ: Tab. 6 compares\nthe performance of the full ACTIVE-PC with its variant,\nwhich excludes the EEQ module, under different neighbor-\nhood radius settings. Traditional neighborhood query meth-\nods are constrained by fixed neighborhood radii, whereas\nEEQ adapts by learning the intrinsic geometric properties\nof the data distribution, allowing for flexible scaling of the\nspherical neighborhood, thereby enhancing robustness. The\nvariance of accuracy (acc) for ACTIVE-PC across different\nneighborhood radii is 0.100, significantly outperforming the\nvariant without EEQ, which exhibits a variance of 0.456.\n5.5. Visualization and Analysis\nWe utilize t-SNE [25] visualization to analyze feature dis-\ntributions from the point cloud modality, as shown in\nFig. 4. ACTIVE-PC distinctly separates similar actions\n(e.g., \u201dRaising Arms\u201d vs. \u201dScratching Head\u201d and \u201dArms\nCrossed\u201d vs. \u201dHands on Waist\u201d), whereas PST-Transformer\n[6] produces more overlapping clusters. This demonstrates\nACTIVE-PC\u2019s superior ability to differentiate subtle action\ncategories from a robotic viewpoint.\n6. Conclusion\nTo the best of our knowledge, our dataset is the first human\naction recognition dataset specifically designed for Natural\nHuman-Robot Interaction scenarios, laying the foundation\nfor more accurate and detailed understanding of human ac-\ntions in N-HRI. We also propose ACTIVE-PC designed for\naction recognition in N-HRI scenarios. Experimental re-\nsults demonstrate the effectiveness of this approach.\n\n--- Page 9 ---\nAcknowledgements This work was supported by National\nNatural Science Foundation of China (No. 62473007),\nNatural Science Foundation of Guangdong Province (No.\n2024A1515012089), Shenzhen Innovation in Science and\nTechnology Foundation for The Excellent Youth Scholars\n(No. RCYX20231211090248064).\nReferences\n[1] Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun\nChen, Hongbo Fu, and Chiew-Lan Tai. Transfusion: Robust\nlidar-camera fusion for 3d object detection with transform-\ners. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition , 2022. 7\n[2] Yizhak Ben-Shabat, Oren Shrout, and Stephen Gould. 3di-\nnaction: Understanding human actions in 3d point clouds.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , 2024. 7\n[3] Elif Bozkaya-Aras, Tolga Onel, Levent Eriskin, and Mumtaz\nKaratas. Intelligent human activity recognition for healthcare\ndigital twin. Internet of Things , 2025. 1\n[4] Haoyu Chen, Henglin Shi, Xin Liu, Xiaobai Li, and Guoying\nZhao. Smg: A micro-gesture dataset towards spontaneous\nbody gestures for emotional stress state analysis. Interna-\ntional Journal of Computer Vision , 2023. 3\n[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ArXiv , 2020. 8\n[6] Hehe Fan, Yi Yang, and Mohan Kankanhalli. Point 4d trans-\nformer networks for spatio-temporal modeling in point cloud\nvideos. In 2021 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2021. 3, 7, 8\n[7] Hehe Fan, Xin Yu, Yuhang Ding, Yi Yang, and MohanS.\nKankanhalli. Pstnet: Point spatio-temporal convolution on\npoint cloud sequences. Cornell University - arXiv,Cornell\nUniversity - arXiv , 2022. 7\n[8] Hehe Fan, Xin Yu, Yi Yang, and Mohan Kankanhalli. Deep\nhierarchical representation of point cloud videos via spatio-\ntemporal decomposition. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 2022. 7\n[9] Hehe Fan, Yi Yang, and Mohan Kankanhalli. Point spatio-\ntemporal transformer networks for point cloud video mod-\neling. IEEE Transactions on Pattern Analysis and Machine\nIntelligence , 2023. 3, 7\n[10] Shiming Ge, Jia Li, Qiting Ye, and Zhao Luo. Detecting\nmasked faces in the wild with lle-cnns. In 2017 IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\n2017. 1\n[11] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu\nSong, Yu Liu, Hongsheng Li, and Yu Jiao Qiao. Uniformer:\nUnifying convolution and self-attention for visual recogni-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence , 2022. 3, 7[12] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\nLimin Wang, and Yu Qiao. Uniformerv2: Unlocking the po-\ntential of image vits for video understanding. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV) , 2023. 7\n[13] Wanqing Li, Zhengyou Zhang, and Zicheng Liu. Action\nrecognition based on a bag of 3d points. In 2010 IEEE\ncomputer society conference on computer vision and pattern\nrecognition-workshops , 2010. 2, 3\n[14] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man-\ngalam, Bo Xiong, Jitendra Malik, and Christoph Feicht-\nenhofer. Mvitv2: Improved multiscale vision transform-\ners for classification and detection. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , 2022. 7\n[15] Xin Liu, Henglin Shi, Haoyu Chen, Zitong Yu, Xiaobai Li,\nand Guoying Zhao. imigue: An identity-free video dataset\nfor micro-gesture understanding and emotion analysis. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition , 2021. 3\n[16] Yunze Liu, Junyu Chen, Zekai Zhang, Jingwei Huang, and Li\nYi. Leaf: Learning frames for 4d point cloud sequence un-\nderstanding. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2023. 3\n[17] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , 2022. 7\n[18] I Loshchilov. Decoupled weight decay regularization. 2017.\n8\n[19] Marcus Mast, Michael Burmester, Birgit Graf, Florian Weis-\nshardt, Georg Arbeiter, Michal Spanel, Zden \u02c7ek Materna,\nPavel Smrz, and Gernot Kronreif. Design of the human-robot\ninteraction for a semi-autonomous service robot to assist el-\nderly people. 2015. 1\n[20] Yang Mi and Song Wang. Recognizing micro actions in\nvideos: learning motion details via segment-level temporal\npyramid. In 2019 IEEE International Conference on Multi-\nmedia and Expo (ICME) , 2019. 3\n[21] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\nNtu rgb+ d: A large scale dataset for 3d human activity anal-\nysis. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition , 2016. 2, 3\n[22] Zhiqiang Shen, Xiaoxiao Sheng, Longguang Wang, Yulan\nGuo, Qiong Liu, and Xi Zhou. Pointcmp: Contrastive mask\nprediction for self-supervised learning on point cloud videos.\nIn2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR) , 2023. 3, 7\n[23] Xiaoxiao Sheng, Zhiqiang Shen, Gang Xiao, Longguang\nWang, Yu Kuen Guo, and Hehe Fan. Point contrastive pre-\ndiction with semantic clustering for self-supervised learning\non point cloud videos. 2023 IEEE/CVF International Con-\nference on Computer Vision (ICCV) , 2023. 7\n[24] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.\nVideomae: Masked autoencoders are data-efficient learners\nfor self-supervised video pre-training. Advances in neural\ninformation processing systems , 2022. 3, 7\n\n--- Page 10 ---\n[25] Laurens Van der Maaten and Geoffrey Hinton. Visualiz-\ning data using t-sne. Journal of machine learning research ,\n2008. 8\n[26] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jun-\ngong Han, and Guiguang Ding. Yolov10: Real-time end-\nto-end object detection. arXiv preprint arXiv:2405.14458 ,\n2024. 8\n[27] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nnetworks for action recognition in videos. IEEE transactions\non pattern analysis and machine intelligence , 2018. 3\n[28] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yi-\nnan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2:\nScaling video masked autoencoders with dual masking. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , 2023. 7\n[29] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun\nHuang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun\nWang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali\nWang, Limin Wang, and Yu Qiao. Internvideo: General\nvideo foundation models via generative and discriminative\nlearning. ArXiv , 2022. 3\n[30] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He,\nGuo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong\nShi, et al. Internvideo2: Scaling foundation models for mul-\ntimodal video understanding. In European Conference on\nComputer Vision , 2024. 7\n[31] Ping Wei, Yibiao Zhao, Nanning Zheng, and Song-Chun\nZhu. Modeling 4d human-object interactions for event and\nobject recognition. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision , 2013. 3\n[32] Sravani Yenduri, Nazil Perveen, Vishnu Chalavadi, and C\nKrishnaMohan. Fine-grained action recognition using dy-\nnamic kernels. Pattern Recognit. , 2022. 1\n[33] Ryo Yonetani, Kris M Kitani, and Yoichi Sato. Recognizing\nmicro-actions and reactions from paired egocentric videos.\nInProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , 2016. 3\n[34] Jianqi Zhong, Kaichen Zhou, Qingyong Hu, Bing Wang,\nNiki Trigoni, and A. Markham. No pain, big gain: Classify\ndynamic point cloud sequences with static models by fitting\nfeature-level space-time surfaces. 2022 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\n2022. 3",
  "project_dir": "artifacts/projects/enhanced_cs.RO_2507.22522v1_Recognizing_Actions_from_Robotic_View_for_Natural_",
  "communication_dir": "artifacts/projects/enhanced_cs.RO_2507.22522v1_Recognizing_Actions_from_Robotic_View_for_Natural_/.agent_comm",
  "assigned_at": "2025-07-31T21:33:28.076903",
  "status": "assigned"
}